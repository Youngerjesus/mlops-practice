# Pytorch 

#### 소개:
- Meta 에서 만든 머신러닝 프레임워크임. 
- Tensor 연산을 효율적으로 다룰 수 있고, CUDA 를 이용해서 GPU 를 이용한 연산도 효율적으로 다룰 수 있다. 
- 파이토치 생태계에서는 TorchVision, TorchAudio 도 포함되어 있어서 주요 딥러닝 신경망도 편리하게 이용할 수 있음.
- 국내에서는 Pytorch 를 Tensorflow 보다 더 많이 사용하고 있다고 함. 

#### Q) CUDA 를 이용한 GPU 연산은 텐서를 효율적으로 처리하기 보다는 행렬을 효율적으로 처리하기 위한거지?

아니다. 

CUDA 는 행렬 연산 뿐 아니라 텐서 연산에서도 최적화 되어 있음. 

#### Pytorch 학습 순서: 
- 1. Tensor: Pytorch 의 Tensor 개념을 이해하고, Tensor 연산을 실습해보는 것  
- 2. nn.Module: 신경망 구성을 위한 모듈을 이해하고 MNIST 샘플 신경망을 구성해보는 것.  
- 3. Training with Pytorch: MNIST 학습 데이터셋을 이용해서 모델을 학습하고, 학습된 모델을 평가해보는 것.  
- 4. TorchServe: 학습한 모델을 추론해보고, TorchServer 를 통해 모델을 서빙해보는 것.  
- 5. 최적화 알고리즘: SGD, Adam 알고리즘을 이해하고, Cross Entropy, MSE 등의 Loss Function 을 이해해보는 것. 

#### Q) MNIST 샘플 신경망이 뭐지? 

MNIST 샘플 신경망은 손으로 쓴 숫자를 인식하는 문제에 대한 표준적인 기계 학습 모델 예제 중 하나임. 

MNIST 데이터셋은 0부터 9까지의 숫자를 손으로 쓴 70,000개의 작은 그레이스케일 이미지로 구성되어 있음. 

이 데이터셋은 머신러닝과 딥러닝의 세계에서 "Hello World" 프로그램과 같은 역할을 함.

MNIST 샘플 신경망은 다음과 같은 구성 요소를 포함하고 있다: 
- 입력층(Input Layer): 이 층은 28x28 픽셀의 이미지를 입력으로 받아, 이를 784개의 뉴런(각 픽셀당 하나의 뉴런)으로 변환하여 신경망에 공급한다. 
- 은닉층(Hidden Layers): 하나 이상의 은닉층이 있을 수 있으며, 각 층은 여러 뉴런으로 구성된다. 뉴런들은 가중치와 활성화 함수를 통해 입력받은 신호를 처리한다. 일반적으로 이러한 층에서는 ReLU(Rectified Linear Unit)와 같은 비선형 활성화 함수가 사용된다. 
- 출력층(Output Layer): 이 층은 10개의 뉴런으로 구성되어 있으며, 각 뉴런은 하나의 숫자(0에서 9까지)에 해당한다. 출력층은 보통 softmax 활성화 함수를 사용하여, 각 숫자에 대한 확률을 출력한다. 
- 손실 함수(Loss Function): 모델의 예측과 실제 라벨 사이의 차이를 측정하기 위해 사용된다. 분류 문제에서는 일반적으로 크로스 엔트로피 손실 함수가 사용된다. 
- 최적화 알고리즘(Optimizer): 손실 함수를 최소화하기 위해 가중치를 조정하는 방법을 정의한다. 대표적인 예로는 SGD(Stochastic Gradient Descent), Adam 등이 있다. 

#### Q) MNIST 에서 최적화 알고리즘은 뭐야? 경사하강법 같은건가? 

맞다. 경사 하강법의 변형들임. 

네트워크의 가중치를 조정하여 손실 함수의 값을 최소화하는 방향으로 학습을 진행한다. 

SGD (Stochastic Gradient Descent) 는  가장 기본적인 형태의 경사 하강법임. 각 반복에서 무작위로 선택된 하나의 데이터 샘플(또는 작은 배치)을 사용하여 가중치를 업데이트한다. 이 방식은 계산 효율성이 높으며, 많은 데이터에도 확장이 용이함.


#### Tensor 종류: 
- 단일 값이 스칼라, 리스트 값이 벡터, 행렬 값이 매트릭스, 더 나아가 3D 구조로 된 3D Tensor or Cube 이렇게 있다.  


#### Q) Python 에 존재하는 리스트 타입 같은 걸 이용하지 않고 Tensor 단위로 사용하는 이유는? 

Tensor 는 하드웨어 도움을 받아서 한번에 처리할 수 있어서. 

CUDA 를 이용하면 행렬을 한번에 계산할 수 있다. 

#### Tensor 계산은 그래프를 통해서 이뤄진다: 
- PyTorch가 연산을 수행할 때 내부적으로 "계산 그래프"(computation graph)를 구성하고 사용한다는 의미임. 
- 계산 그래프는 수행할 연산과 연산에 관여하는 텐서들 간의 관계를 노드와 엣지로 표현한 그래프 구조이다. 
- 이 방식은 특히 딥러닝에서 역전파 알고리즘을 통해 그래디언트를 효율적으로 계산할 때 중요한 역할을 한다고 함:
- 그래프 구조이기 때문에 최종 연산 결과인 텐서에서도 어떻게 이 결과가 나왔는지 추적할 수 있다.

#### Torch 의 nn.Module 클래스: 
- 레이어나 신경망을 정의할 때 사용하는 기본 클래스이다. 
- nn.Module 은 내부적으로 모델의 파라미터를 자동으로 추적하는 기능이 있다. 그리고 parameters() 메소드를 토해 모델의 파라미터를 반환할 수 있음.
- 복잡한 신경망은 여러개의 레이어를 가질 수 있는데 이건 nn.Module 객체가 여러개 있다는 거임. 
- forward() 메소드를 통해 전방 계산을 할 수 있다. 즉 입력 데이터를 모델에 주면 결과를 출력해볼 수 있는거임. 
- nn.Module 은 .train() 과 .eval() 메소드를 통해서 모델의 학습 모드와 평가 모드를 전화할 수 있다. 
- 그리고 .save() 와 .load() 메소드를 통해 모델을 저장하고 로드할 수 있다.

#### 간단한 신경망 정의 방법 
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(10, 50)  # 입력 특성 10개, 출력 특성 50개
        self.fc2 = nn.Linear(50, 2)   # 입력 특성 50개, 출력 특성 2개

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

#### 레이어를 많이 쌓으면 일어나는 일: 
- 어렵고 복잡한 패턴을 파악하기 쉬워진다. 
- 계산양이 많아지고, 추론속도가 느려진다. 


#### Q) nn.Module 의 Full Connected 는 뭐야? 

nn.Module에서 "Fully Connected" 층은 주로 nn.Linear 클래스를 사용하여 구현된다. 

Fully Connected" 층, 또는 밀집층(dense layer)이라고도 불리는 이 층은 신경망에서 모든 입력 노드가 다음 층의 모든 노드와 연결되어 있는 네트워크 구조를 의미한다. 



#### Q) nn.Module 클래스에서 nn 이 붙은 이유는 뭐야? 

PyTorch에서 nn은 "neural networks"의 약자로, 신경망을 구성하는 데 필요한 다양한 도구와 레이어를 포함하는 모듈임. 

nn은 PyTorch의 하위 모듈 중 하나로, 주로 신경망을 구성하는 레이어, 활성화 함수, 손실 함수 등을 제공한다. 이 모듈을 통해 사용자는 딥러닝 모델을 보다 쉽게 구축하고 관리할 수 있다. 


#### Q) nn.Linear 은 그럼 nn.Module 을 상속한 클래스야? 

맞다. 

nn.Linear는 신경망에서 가장 기본적인 레이어 중 하나로, 선형 변환을 수행하는 레이어임. 이 레이어는 입력 데이터에 가중치 행렬을 곱하고 편향을 더해서 출력을 생성한다. 

nn.Linear의 주요 구성: 
- 가중치(weights): 각 입력 특성과 출력 뉴런 사이의 연결 강도를 나타내는 매개변수임. 
- 편향(biases): 각 출력 뉴런에 추가되는 상수 값으로, 출력을 조절하는 역할을 한다. 

nn.Linear 클래스는 다음과 같은 파라미터를 필요로 함. 
- in_features: 입력 특성의 수.
- out_features: 출력 특성의 수.
- bias: 편향을 사용할지 여부를 나타내는 불리언 값. 기본값은 True임. 


```python
import torch.nn as nn

# 입력 특성이 10개, 출력 특성이 5개인 선형 레이어 생성
linear_layer = nn.Linear(in_features=10, out_features=5)

```

#### Q) 드롭아웃과 배치 최적화는 하이퍼파라미터 튜닝이지? 이게 뭐야? 

드롭아웃과 배치 정규화는 신경망을 트레이닝할 때 사용하는 기술임. 

이들의 설정값을 조절하는 것은 하이퍼파라미터 튜닝의 일부이다.

하이퍼파라미터 튜닝은 모델의 성능을 최적화하기 위해 학습 과정에서 조정할 수 있는 매개변수들을 실험적으로 최적화하는 과정이다.

드롭아웃(Dropout):
- 드롭아웃은 과적합을 방지하기 위해 신경망 학습 과정 중에 무작위로 일부 뉴런을 비활성화(즉, 그 뉴런의 출력을 0으로 설정)하는 기술임. 
- 이 방법은 네트워크가 일부 특정 뉴런에 과도하게 의존하는 것을 막고, 다양한 뉴런의 조합을 통해 데이터를 학습하게 함으로써 일반화 능력을 향상시키기 위한 것. 
- 드롭아웃 비율은 하이퍼파라미터로, 일반적으로 0.2에서 0.5 사이에서 설정한다. 

배치 정규화(Batch Normalization): 
- 배치 정규화는 신경망의 각 층에서 활성화 출력을 정규화(평균을 0으로, 분산을 1로 조정)하여 학습 과정을 안정화하고 가속화하는 기술임. 
- 이 기술은 내부 공변량 변화(internal covariate shift)를 줄여주며, 이는 학습 과정에서 각 층의 입력 분포가 변하는 문제를 완화시키는 효과가 있다. 
- 배치 정규화는 각 층의 입력 데이터를 정규화한 후 학습 가능한 파라미터를 통해 스케일과 시프트를 조정한다. 


#### Q) 내부 공변량 변화는 뭐지?

내부 공변량 변화(Internal Covariate Shift)는 신경망을 학습할 때 발생할 수 있는 문제 중 하나임. 

신경망의 각 층을 거치면서 입력 데이터의 분포가 계층을 지나면서 계속 변한다는 개념이다. 

즉, 네트워크의 각 층이 학습 과정 중에 계속 변화하는 데이터 분포에 적응해야 하는 상황을 말한다. 

내부 공변량 변화로 인한 영향은 다음과 같다: 
- 학습 속도 저하: 각 층의 입력 분포가 계속 변하면, 각 층의 가중치를 적절히 조절하는 것이 더 어려워집니다. 이는 네트워크가 더 천천히 학습되게 만들 수 있다. 
- 과적합 위험 증가: 네트워크의 깊은 층이 이전 층의 변화에 민감하게 반응할 때, 모델이 훈련 데이터의 노이즈에 더 쉽게 영향을 받을 수 있다. 
- 학습 안정성 감소: 입력 분포의 지속적인 변화는 모델의 수렴을 더 어렵게 만들고, 때로는 학습이 불안정해질 수 있다. 



#### Q) 배치 정규화에서 활성화 출력을 정규화 하는 것이 왜 학습 과정을 안정화시키는거지? 

배치 정규화에서 활성화 출력을 정규화하는 것이 학습 과정을 안정화시키는 데 도움이 되는 몇 가지 주요 이유가 있다: 
- 가중치 초기화 의존성 감소: 일반적으로 신경망을 훈련할 때 가중치 초기화가 매우 중요하다. 잘못된 초기화는 네트워크의 학습을 느리게 하거나 전혀 학습되지 않게 만들 수 있다. 배치 정규화는 각 층의 입력을 정규화하여, 네트워크의 초기 가중치에 대한 의존성을 줄이고, 더 넓은 범위의 초기 가중치 설정에서도 모델이 잘 작동할 수 있도록 한다.
- 학습률 증가: 배치 정규화는 각 층의 입력 분포를 안정화시킴으로써, 높은 학습률에서도 모델이 수렴할 가능성을 높인다. 높은 학습률을 사용할 수 있게 되면, 그만큼 학습 과정이 빨라지고 더 빠르게 좋은 성능에 도달할 수 있다.
- 내부 공변량 변화 억제: 배치 정규화는 각 층의 입력을 정규화하여 그 분포를 비슷하게 유지함으로써, 학습 동안 네트워크 파라미터의 작은 변경이 출력에 미치는 영향을 줄인다. 이는 내부 공변량 변화를 감소시키며, 결과적으로 각 층이 더 안정적으로 학습될 수 있게 한다. 
- 그라디언트 소실 및 폭발 문제 완화: 심층 네트워크에서는 그라디언트 소실 또는 폭발이 자주 발생할 수 있는데, 이는 그라디언트가 네트워크를 통과하면서 점차 사라지거나 지나치게 커지는 현상이다. 배치 정규화는 각 층의 활성화를 적절한 범위 내에서 유지함으로써 이러한 문제를 완화시킬 수 있다. 
- 과적합 방지: 배치 정규화는 미니 배치마다 평균과 분산을 계산하는 과정에서 일종의 노이즈를 추가하게 되는데, 이는 모델이 더 강인해지게 만들어 과적합을 방지하는 효과를 가질 수 있다. 

#### Q) 학습 과정이 안정화된다는 건 학습 과정이 길지 않다는 걸 의미하는건가? 아니면 전역 최소점에 도달할 수 있음을 의미하는건가? 

학습 과정이 안정화된다는 건 다음 두 가지 요소를 모두 포함하는 개념이다: 
- 학습 과정의 속도: 학습 과정이 안정화되면 일반적으로 학습 속도가 빨라진다. 이는 모델이 학습 데이터에 대해 더 효과적으로 학습하고, 더 빨리 수렴할 수 있음을 의미한다. 즉, 모델이 최적의 가중치를 찾아가는 데 필요한 에포크 수가 감소할 수 있다. 
- 수렴의 질: 또한, 안정화된 학습 과정은 모델이 전역 최소점에 더 가까워질 가능성을 높인다. 이는 네트워크가 보다 일관된 학습 패턴을 따르고, 이상적인 학습률과 초기화 조건 하에서 최적의 결과를 달성할 수 있음을 의미한다. 여기서 전역 최소점이란, 손실 함수가 가능한 가장 낮은 값을 가지는 지점을 말한다. 


#### Q) 배치 정규화를 하지 않아서 내부 공변량이 일어나서 각 층의 입력 분포가 변하는게 학습에 불안정한 영향을 주는 것 같은데 왜 불안정한 영향을 주는거지? 수렴하는 방향으로 업데이트가 되지 않아서 그런거야? 

맞다. 

내부 공변량 변화가 발생하면 각 층의 입력 분포가 학습 과정 동안 계속 변화하게 되어, 이는 다양한 방식으로 학습에 불안정한 영향을 줄 수 있다. 

1) 가중치 업데이트의 복잡성 증가: 

각 층의 입력 분포가 계속 변하면, 각 층의 가중치를 적절히 조정해야 하는 필요성이 증가한다. 

이 때문에 네트워크는 적절한 가중치 업데이트를 찾기 위해 더 많은 학습을 요구하게 되고, 이는 학습 과정을 느리게 할 수 있다. 

가중치가 적절히 조정되지 않으면, 네트워크는 수렴하지 않거나 예상치 못한 방향으로 수렴할 수 있다. 


2) 그라디언트 소실 및 폭발

내부 공변량 변화는 특히 깊은 신경망에서 문제를 일으킬 수 있다. 

입력 분포의 변화로 인해 활성화 함수를 통과한 후의 그라디언트가 소실되거나 폭발할 수 있다. 

예를 들어, 신경망에서 사용되는 시그모이드나 탄젠트와 같은 활성화 함수는 입력 값이 너무 크거나 작을 때 그라디언트가 매우 작아지는 현상(그라디언트 소실)을 겪을 수 있다. 

이는 깊은 층으로 갈수록 학습이 거의 이루어지지 않게 만들 수 있다. 


#### Q) 입력 분포가 계속 변하면 각 층의 가중치를 조절하는게 어렵다고 하고, 적절한 가중치를 업데이트 하기 위해 더 많은 학습을 요구한다고 하는데 이 부분에 대해서 이해가 안가서 쉽게 설명해줘.

신경망에서 각 층은 입력 데이터에 대해 특정 변환(가중치와 활성화 함수를 사용한 연산)을 수행한다. 

각 층의 출력은 다음 층의 입력이 됩니다. 만약 한 층의 입력 분포가 계속 변한다면, 그 층이 학습해야 할 패턴 또한 계속 변하게 된다. 

이는 각 층이 끊임없이 변화하는 데이터에 적응하기 위해 계속해서 다른 방식으로 조정되어야 함을 의미한다. 

예를 들어 레스토랑에서 요리사가 특정 요리법에 따라 요리를 만들어야 한다고 생각해보자. 

만약 재료의 맛이나 질감이 매번 달라진다면, 요리사는 매번 요리 방법을 조금씩 조정해야 한다. 이와 유사하게, 신경망의 각 층도 입력 데이터의 변화에 맞추어 가중치를 계속 조정해야 한다. 


#### 실습: XORModel 

XOR 문제는 선형적으로 구분할 수 없는 문제로 알려져 있기 때문에, 이를 해결하기 위해 여기서는 비선형 활성화 함수인 시그모이드를 사용하는 두 개의 레이어를 사용한다. 

```python
class XORModel(nn.Module): 
    def __init__(self):
        super(XORModel, self).__init__()
        self.layer1 = nn.Linear(2, 2) # 입력 2개, 출력 2개인 레이어 1 
        self.layer2 = nn.Linear(2, 1) # 입력 2개, 출력이 1개인 레이어 2 
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        x = self.sigmoid(self.layer1(x))
        x = self.sigmoid(self.layer2(x))
        return x
```
- self.sigmoid = nn.Sigmoid(): 시그모이드 활성화 함수를 인스턴스한다. 시그모이드는 각 노드의 출력을 0과 1 사이로 제한하는 비선형 함수로, 이진 분류 문제에 적합하다. 
- forward 함수는 모델에 데이터를 전달할 때 호출되는 함수입니다. 입력 데이터 x를 받아 순전파 과정을 통해 최종 출력을 계산한다. 
- x = self.sigmoid(self.layer1(x)): 첫 번째 선형 층 layer1을 통해 입력 x가 전달되고, 그 결과는 시그모이드 함수를 통해 비선형 변환된다. 이 단계는 입력 데이터에 초기 변환을 적용하고 비선형성을 추가하여 다음 레이어로 전달한다. 
- x = self.sigmoid(self.layer2(x)): 두 번째 선형 층 layer2를 통해 데이터가 전달되고, 그 결과 또한 시그모이드 함수를 통해 비선형 변환된다. 이는 최종적으로 0 또는 1의 출력을 생성한다.


#### Q) Pytorch 로 문제를 해결하는 딥러닝 모델을 만들 때는 이렇게 forward 함수를 늘 정의해야해? 각 레이어에서 어떻게 계산하는지 같은 것들을? 

PyTorch에서 사용자 정의 신경망 모델을 만들 때는 일반적으로 forward 함수를 정의해야 한다. 

이 함수는 모델의 순전파(forward propagation) 단계를 정의하며, 모델이 어떻게 입력 데이터를 받아들이고, 그 데이터를 여러 신경망 레이어와 활성화 함수를 거쳐서 출력을 생성하는지 설명하는 부분이다. 

forward 함수는 PyTorch가 모델을 훈련하거나 평가할 때 자동으로 호출되며, 개발자는 이 함수를 통해 데이터가 모델을 통해 어떻게 흐르는지 제어할 수 있다. 모델의 복잡도나 특성에 따라 forward 함수 내의 로직은 단순하거나 매우 복잡할 수 있다. 


#### Q) 대부분의 경우에  사용자 정의 신경망 모델을 사용해? 아니면 사용자 정의 신경망 모델을 사용하지 않는 경우도 있어? CNN, RNN 이런걸 사용하는 건 사용자 정의 신경망 모델이 아닌건가? 

일반적인 신경망 아키텍처(CNN, RNN, LSTM 등)를 위한 미리 구현된 모듈을 제공하며, 이러한 표준 모듈들을 사용하여 많은 일반적인 문제를 해결할 수 있다. (즉 사용자 정의 신경망 모델이 아닌거지) 
- 이미지 분류, 시퀀스 예측, 텍스트 처리와 같은 일반적인 문제는 CNN, RNN, LSTM 등과 같은 표준 아키텍처를 사용하여 효과적으로 해결할 수 있다. 
- 이미지 처리에는 주로 CNN을 사용하고, 시퀀스 데이터나 시계열 데이터에는 RNN이나 LSTM을 사용한다. 

특정 작업에 맞추어 복잡한 아키텍처를 구현해야 할 때 사용자 정의 모델이 필요할 수 있다. 

예를 들어, 특정 유형의 데이터를 처리하기 위해 비표준 데이터 흐름이나 특별한 계산 레이어를 필요로 하는 경우임. 

#### Q) 이전 예제에서 sigmoid 함수를 정의한 이유는 입력이 주어졌을 때 어떻게 계산할 지 미리 정해놓는건가? 딥러닝에서는 이렇게 해야하는건가? 이것말고 다른 함수들은 무엇이 있지?
 
sigmoid 같은 활성화 함수는 각 레이어의 출력에 적용되어 다음 레이어로 전달되기 전에 비선형 변환을 수행한다. 

이런 활성화 함수를 사용하는 주요 목적은 네트워크에 비선형성을 추가하는 것임. 

활성화 함수의 종류와 특성:
- 시그모이드 (Sigmoid):
  - 범위: 0과 1 사이.
  - 특징: 주로 이진 분류 문제의 마지막 레이어에서 출력을 확률로 표현할 때 사용한다. 
  - 단점: 그라디언트 소실 문제(vanishing gradient)가 발생할 수 있고, 출력이 0 또는 1에 가까울 때 그라디언트가 매우 작아집니다.

- 하이퍼볼릭 탄젠트 (Tanh):
  - 범위: -1과 1 사이.
  - 특징: 중심이 0으로 이동되어 있어 학습 초기 단계에서 시그모이드보다 선호된다. 
  - 단점: 그라디언트 소실 문제가 여전히 존재한다. 

- 렐루 (ReLU - Rectified Linear Unit):
  - 범위: 0에서 무한대.
  - 특징: 연산이 간단하며, 많은 경우에 좋은 성능을 보여주어 현재 가장 널리 사용되는 활성화 함수임. 
  - 단점: 죽은 렐루 문제(dead ReLU problem), 즉 일부 뉴런이 0 이하의 입력에서 활성화되지 않고 그라디언트도 전달되지 않는 문제가 발생할 수 있습니다.

- 리키 렐루 (Leaky ReLU):
  - 범위: 모든 실수.
  - 특징: 죽은 렐루 문제를 해결하기 위해 음수 입력에 대해 매우 작은 기울기를 부여한다. 
  - 사용: ReLU의 변형으로 널리 사용되고 있다. 

- 소프트맥스 (Softmax):
  - 범위: 0과 1 사이, 전체 출력 합은 1.
  - 특징: 주로 다중 클래스 분류 문제의 출력 레이어에서 사용된다. 클래스 확률을 출력하기 위해 사용된다. 
  - 사용: 각 클래스에 대한 확률을 계산하여 가장 높은 확률을 가진 클래스를 모델의 예측으로 선택한다. 




#### Q) 신경망 네트워크에 비선형성을 추가하는 이유는 뭐야? 대부분의 문제는 비선형성을 띄어서 그런건가?  

맞다. 

실제 세계의 데이터와 문제들이 대부분 비선형 특성을 지니기 때문임. 

비선형 활성화 함수를 통해 신경망은 복잡한 함수를 근사할 수 있다. 이는 신경망이 단순한 선형 모델로 표현할 수 없는 복잡한 결정 경계와 관계를 학습할 수 있게 해준다. 

비선형 활성화 함수를 사용함으로써 신경망은 비선형 데이터에 대한 더 좋은 일반화와 예측 성능을 보여줄 수 있다. 선형 모델은 데이터의 복잡한 패턴을 포착하기에 한계가 있음. 



#### Q) 그라디언트 소실 문제가 뭔데? 더 나아가서 시그모이드 활성화 함수를 사용할 때 발생하는 그라디언트 소싦 문제는 또 뭔데?  

그라디언트 소실 문제(vanishing gradient problem)는 딥러닝 모델, 특히 깊은 신경망을 학습할 때 발생할 수 있는 중대한 문제 중 하나임. 

신경망의 가중치를 업데이트하는 과정에서 발생하며, 신경망의 뒤쪽 레이어에서부터 앞쪽 레이어로 그라디언트(오류의 기울기)가 전파되면서 점점 감소하는 현상을 말한다. 

이렇게 그라디언트가 점점 작아지면 앞쪽 레이어의 가중치는 거의 업데이트되지 않게 되어, 네트워크 학습이 효과적으로 이루어지지 않는다. 

시그모이드 활성화 함수와 그라디언트 소실 문제: 
- 시그모이드 활성화 함수는 출력 값이 0에서 1 사이로 제한되며, 함수의 형태가 S자 모양입니다. 이 활성화 함수의 수학적 표현은 다음과 같다:
![](./images/sigmoid.png)

- 시그모이드 함수의 문제점은 그라디언트를 계산할 때 발생합니다. 시그모이드 함수의 도함수는 다음과 같다:
![](./images/sigmoid%20도함수.png)

- 이 도함수의 값은 입력값 x가 0에 가깝거나 매우 크거나 매우 작을 때 매우 작아지는 특성잉 있다. 

- 이는 네트워크가 깊어질수록 또는 연속적인 계산에서 입력값이 클 경우 그라디언트가 점점 작아지는 결과를 초래하며, 결과적으로 네트워크의 앞부분에 위치한 레이어의 가중치는 거의 업데이트되지 않게 된다.

- 이로 인해 신경망 학습이 제대로 이루어지지 않게 되며, 이를 그라디언트 소실 문제라고 함. 

- 이 문제를 완화하기 위해 다양한 방법들이 제안되었음. 가장 흔한 해결책 중 하나는 ReLU(Rectified Linear Unit) 활성화 함수와 그 변형들을 사용하는 것이다. ReLU 함수는 입력이 양수일 경우 그대로 출력하고, 음수일 경우 0을 출력한다. 이 함수는 양수 입력에 대해 그라디언트가 항상 1이므로 그라디언트 소실 문제에 효과적이다. 
- 다른 해결 방안으로는 가중치 초기화 기법 개선, 배치 정규화, 적절한 학습률 스케줄링 등이 있다. 

#### Q) 죽은 렐루 문제(dead ReLU problem) 는 입력값으로 0 미만이 들어와서 ReLU 함수가 호출되지 않는 문제를 말하는거지? 그리고 Leaky ReLU 는 음수 입력에서도 출력을 할 수 있도록 만드는거고. 

맞다. 

ReLU 함수는 입력이 0 이하일 경우 출력을 0으로 만든다.  

따라서, 어떤 뉴런이 한번 0 이하의 입력을 받게 되면, 그 후로는 계속 0을 출력하게 되어, 학습 과정에서 그 뉴런이 더 이상 업데이트되지 않는 상황이 발생한다. 

#### Q) 신경망 네트워크에서 레이어는 뒤쪽에서 앞쪽 방향으로 업데이트 되는건가? 그래서 Backpropagation 이라고 부르는건가?

맞다. 

신경망에서의 가중치 업데이트는 일반적으로 역전파(Backpropagation) 알고리즘을 통해 이루어진다. 

역전파는 신경망의 출력층에서부터 입력층 방향으로 오차의 그라디언트를 전파하면서 각 레이어의 가중치를 업데이트하는 과정이다. 

역전파의 주요 단계:
- 1. 전방 전파(Forward Propagation):
  - 입력 데이터는 입력층에서부터 출력층까지 순차적으로 각 레이어를 통과한다.  
  - 각 레이어는 가중치와 활성화 함수를 사용하여 입력을 처리하고 다음 레이어로 결과를 전달한다.  
  - 최종적으로, 출력층에서 네트워크의 예측 결과가 생성된다. 
- 2. 오차 계산:
  - 출력층에서 예측된 결과와 실제 타깃(레이블) 사이의 오차를 계산한다.  이 오차는 손실 함수(loss function)를 사용하여 수치화된다. 
- 3. 역방향 전파(Backward Propagation):
  - 계산된 오차를 바탕으로, 출력층에서부터 시작하여 입력층의 방향으로 그라디언트(가중치에 대한 오차의 편미분)를 계산한다. 
  - 이 과정은 연쇄 법칙(chain rule)을 사용하여 각 레이어의 가중치에 대한 오차의 기여도를 찾아낸다. 
  - 각 레이어의 가중치는 이 그라디언트를 사용하여 업데이트된다.  이때 학습률(learning rate)이라는 매개변수가 그라디언트의 크기를 조절하여 가중치의 조정 정도를 결정한다. 

- 4. 반복 과정:
  - 이 전체 과정은 네트워크의 성능이 향상될 때까지, 즉 손실이 최소화될 때까지 반복된다. 


#### Q) 정방향을 사용하지 않고 역방향으로 가중치를 업데이트 하는 이유는 뭐야?  

출력층의 오류를 잡는 것이 더 중요해서. 

출력층에서 계산된 오류는 모델의 예측이 실제 값과 얼마나 차이가 나는지를 직접적으로 나타낸다. 


#### Q) 다음 Pytorch 의 모델 초기화 코드는 뭐지? 

```python
# 모델 초기화 
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
```
- 손실함수 설정: nn.BCELoss()는 Binary Cross Entropy Loss, 즉 이진 크로스 엔트로피 손실 함수를 생성한다. 이 함수는 이진 분류 문제에서 사용되며, 모델이 예측한 확률과 실제 레이블 사이의 차이를 측정한다. 
- optim.SGD는 확률적 경사 하강법(Stochastic Gradient Descent, SGD) 최적화 함수를 생성한다. 이 함수는 모델의 가중치를 업데이트하여 학습 과정에서 손실을 최소화하는 데 사용된다.
- model.parameters()는 최적화할 모델의 파라미터(가중치와 편향)를 가져오는 함수이다.  이는 SGD 알고리즘에서 업데이트될 변수들임. 
- lr=0.1은 학습률(learning rate)을 의미하며, 이는 가중치 업데이트 시 그라디언트의 크기를 조절한다. 학습률이 너무 높으면 학습이 불안정해질 수 있고, 너무 낮으면 학습 속도가 느려질 수 있다. 


#### Q) 다음 Pytorch 의 학습하는 코드는 뭐지? 

```python
for epoch in range(10001): 
    # Forward pass 
    outputs = model(x_data)
    loss = criterion(outputs, y_data)
    
    # Backward pass and optimization 
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if (epoch % 1000 == 0): 
        print(f'Epoch [{epoch/10000}, Loss: {loss.item():.4f}')
```

- epoch는 학습 과정을 통해 데이터를 반복해서 학습시키는 횟수를 나타낸다. 여기서는 총 10,001번의 에폭(epoch) 동안 모델을 학습시킨다. 
- model(x_data): 모델에 입력 데이터 x_data를 넣어 예측 결과 outputs를 받는다. 
- loss = criterion(outputs, y_data): 예측 결과와 실제 레이블 y_data를 손실 함수 criterion에 넣어 계산한다. 이 손실 값은 모델의 예측이 얼마나 잘못되었는지를 나타내며, 이 값을 최소화하는 것이 학습의 목표임. 
- optimizer.zero_grad(): 기존에 계산된 그라디언트 값을 초기화한다. PyTorch는 기본적으로 그라디언트를 누적하기 때문에, 각 학습 단계에서 이를 초기화해주어야 한다. 
- loss.backward(): 손실 함수로부터 그라디언트를 자동으로 계산하여 역방향으로 전파한다. 이 과정에서 각 파라미터의 손실 함수에 대한 미분값이 계산된다. 
- optimizer.step(): 계산된 그라디언트를 사용하여 모델의 파라미터를 업데이트한다. 이는 optim.SGD 등의 최적화 알고리즘을 통해 이루어진다. 



#### Q) 왜 기존에 계산된 그라디언트 값을 epoch 때마다 초기화하는거지? 누적해서 그라디언트 값을 업데이트 해야하는거 아닌가? 

그라디언트 값을 매 에폭마다 초기화하는 이유는 그라디언트가 기본적으로 누적되기 때문임. 

이는 PyTorch 설계의 일부로, 여러 미니배치들에 걸쳐 그라디언트를 누적하는 것을 가능하게 한다. 

하지만 일반적인 학습 시나리오에서는 각 에폭이 독립적인 계산을 대표하므로, 누적된 그라디언트를 초기화하지 않으면 이전 에폭에서 계산된 그라디언트가 현재 에폭의 업데이트에 영향을 미치게 된다. 



#### Q) 손실함수 결과인 loss 를 전달받지 않고, optimizer.step() 만 해도 모델 파라미터를 업데이트 할 수 있는 이유는 뭐야?


loss.backward() 메소드를 호출하면 손실값에 대한 그라디언트 값이 계산되고, 이 값이 .grad 에 저장되고, optimizer 내부에서는 .grad 값을 참조하고 있으니까 저장된 그라디언트값에 접근할 수 있고 모델 파라미터를 업데이트하면 됨. 

이 과정에서 설정된 학습률(learning rate)을 기반으로 파라미터를 조정하게 된다. 

그러니 손실 값 자체는 이 과정에 직접적으로 필요하지 않다. 오직 그 파라미터에 연결된 그라디언트 정보만 사용된다. 

