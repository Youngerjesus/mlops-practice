# Activate Function

#### 활성화 함수가 왜 필요할까? 

![](./images/activate%20function%201.png)
![](./images/activate%20function%202.png)

- 활성화 함수를 사용하지 않고, y=x 그대로 이용하게 되면 선형 함수가 될거임.   

- 그러면 결국에 선형 함수가 된다. 선형 함수로 모델은 일반화를 해서 예측을 한다는건데, 잘 예측할 수 있을까? 아니다.   

- 활성화 함수로는 비선형 함수만을 사용한다. 비선형 활성화 함수가 복잡한 패턴을 잡아주는데 도움을 더 주기 때문임. 그리고  활성화 함수가 선형 함수라면, 아무리 많은 레이어를 쌓더라도 이론적으로는 결국 하나의 선형 함수로 표현이 가능하기 떄문에 레이어가 별로 안중요함. 

![](./images/activation%20function%20compare.png)

***

#### 활성화 함수의 특징 

- 신경망의 레이어가 깊어질수록 활성화 함수의 효과가 극대화 됨: 
  - 선형 함수는 레이어를 여러개 쌓더라도 결국에 단순한 선형이 되는 반면에 비선형 함수는 이전 레이어 출력에 변화를 적용하고 다음 결과를 전달해주는 식이라서 레이어가 깊어질수록 변화가 극대화되는거임. 
  - 그러니까 레이어가 깊어짐에 따라 활성화 함수를 통해 생성되는 비선형성이 쌓이게 됨. 이를 통해 복잡한 문제를 추상화 할 수 있게 도니다. 
- 각 활성함수는 신호 활성화의 기준이 존재한다. 특정 임계치를 초과할 때 신호 강도가 증가한다. (ReLU 를 생각해보면 됨)
- 비선형 함수를 활성화 함수로 적용해야한다. (복잡한 문제를 일반화 하기 위해서)

***

#### 활성화 함수의 종류 

![](./images/activatie%20function%20type.png)

- 선택하는 활성화 함수에 따라 네트워크의 성능과 학습 효율이 크게 달라질 수 있다. 그래서 이 차이에 대해 아는게 중요함.  

- ReLU:
  - 특징: ReLU 함수는 입력이 0 이상이면 입력을 그대로 출력하고, 0 이하면 0을 출력한다.  이는 수학적으로 f(x) = max(0, x)로 표현됨. 
  - 목적: 비선형성을 도입하면서도 계산이 매우 간단하다는 장점이 있음. 또한, 그래디언트 소실 문제를 어느 정도 완화시켜준다. 
  - 사용 시기: 일반적으로 CNN이나 일반적인 딥러닝 네트워크에서 널리 사용되며, 특히 이미지 처리나 분류 작업에 효과적임. 
  - 사용 사례: 
    - 컨볼루션 신경망 (CNN): ReLU는 이미지 인식과 같은 컴퓨터 비전 작업에서 널리 사용된다. CNN 레이어 사이에서 비선형성을 도입하고, 계산 속도를 높여 주기 때문에, 깊은 네트워크에서 효과적으로 활용됨.
    - 심층 신경망: ReLU는 일반적인 심층 신경망에서도 기본적으로 사용되며, 그래디언트 소실 문제를 줄이는 데 도움을 준다. 특히, 큰 데이터셋과 복잡한 모델 구조에서 선호된다. 


- Leaky ReLU: 
  - 특징: Leaky ReLU는 ReLU의 변형으로, 입력이 음수일 때도 아주 작은 기울기를 허용한다. 수학적으로 f(x) = x if x > 0 else αx (α는 작은 상수)로 표현됨. 
  - 목적: ReLU에서 발생할 수 있는 "죽은 뉴런" 문제를 해결하기 위해 고안되었음. 음수 입력에 대해서도 그래디언트가 전파될 수 있도록 함으로써 뉴런이 활성화 상태를 유지할 가능성을 높임.
  - 사용 시기: ReLU를 사용하다가 뉴런의 비활성화 문제가 관찰될 때, 특히 음수 값에 대한 정보가 중요할 때 유용하다. 
  - 사용 사례: 
    - 음성 인식: 음성 데이터는 종종 음수 값도 중요한 정보를 포함하고 있다. Leaky ReLU는 이러한 음수 입력을 완전히 무시하지 않고 작은 기울기를 통해 네트워크 학습에 참여시킨다. 
    - GANs (Generative Adversarial Networks): GANs에서는 생성자와 판별자 모델이 복잡한 데이터 분포를 학습한다. Leaky ReLU는 판별자에서 종종 사용되어, 모델이 더 안정적으로 학습할 수 있도록 돕는다. 

    
- ELU (Exponential Linear Unit): 
  - 특징: ELU는 음수 입력에 대해 지수적 감소를 하는 비선형성을 제공한다. f(x) = x if x > 0 else α(exp(x) - 1) (α는 상수)로 정의됨. 
  - 목적: Leaky ReLU와 비슷하게 음수 입력에서도 그래디언트 소실을 방지하고, 출력값의 평균을 0에 가깝게 유지하여 배치 정규화 효과를 일부 제공한다. 
  - 사용 시기: 빠른 수렴을 요구하거나 음수 입력에서 정보 손실을 방지하고자 할 때 적합하다.
  - 사용 사례: 
    - 자연어 처리 (NLP): NLP 모델에서는 단어의 의미적 특성을 잘 파악하는 것이 중요하다.. ELU는 음수 입력에서도 유의미한 그래디언트를 유지하여 더 깊은 레이어의 정보 손실을 방지한다. 
    - 회귀 문제: ELU의 출력이 음수 값을 포함할 수 있기 때문에, 실수 값을 예측하는 회귀 문제에도 적합하다. 


- Sigmoid: 
  - 특징: Sigmoid 함수는 출력을 0과 1 사이로 압축하는 S자 형태의 함수로, f(x) = 1 / (1 + exp(-x))로 표현된다. 
  - 목적: 주로 출력층에서 이진 분류를 위해 사용되어 확률 같은 값으로 출력을 제한한다. 
  - 사용 시기: 이진 분류 문제의 출력층에 주로 사용되며, 확률을 모델링할 때 유용하다. 
  - 사용 사례: 
    - 이진 분류: 출력층에서 Sigmoid 함수를 사용하여, 예측 결과를 0과 1 사이의 확률로 나타낼 수 있다. 예를 들어, 이메일 스팸 분류나 환자의 질병 유무 판단 등에 사용됩니다.
    - 로지스틱 회귀: Sigmoid 함수는 로지스틱 회귀 모델의 핵심 구성 요소로, 결과를 확률로 해석할 수 있게 해준다. 


- Step function: 
  - 특징: 입력이 0 이상이면 1을, 그 외에는 0을 출력하는 함수임. 
  - 목적: 간단한 이진 결정을 내리는데 사용된다. 
  - 사용 시기: 현대의 딥러닝에서는 거의 사용되지 않음. 

- Tanh: 
  - 특징: 
    - Tanh 함수는 하이퍼볼릭 탄젠트 함수로, 수학적으로는 tanh(x) = (e^x - e^-x) / (e^x + e^-x)로 정의됩니다. 이 함수는 S자 형태를 가지며, 출력 범위는 -1과 1 사이이다.
    - Tanh는 원점(0,0)을 중심으로 대칭인 함수이다. 이 대칭성은 네트워크의 출력을 자동으로 정규화하는 효과를 가지며, 학습 초기 단계에서 더 빠른 수렴을 도울 수 있다. 
    - 값이 크거나 작을수록 기울기가 0인데 이는 신호 강도가 약해짐을 의미한다. 그래서 그라디언트 소실 문제가 발생할 수 있음. 
  - 목적: 
    - 출력 정규화: Tanh 함수의 출력이 -1과 1 사이로 제한됨으로써, 이 함수는 자연스럽게 데이터를 정규화하는 효과를 가진다. 이는 신경망이 특히 그라디언트 기반의 학습 알고리즘을 사용할 때 더 안정적으로 학습을 진행할 수 있게 도와준다. 
    - 정보의 균형 잡힌 표현: 0을 중심으로 대칭이기 때문에, 입력 값의 평균이 0에 가까워지도록 도와줍니다. 이는 학습 과정에서 그라디언트의 흐름을 개선하고, 신경망의 전반적인 효율을 증가시킨다. 
  - 사용 시기: 
    - 순환 신경망 (RNN) 및 LSTM: Tanh는 순환 신경망(RNN)과 장기 단기 기억(LSTM) 네트워크에서 매우 일반적으로 사용된다. 이러한 네트워크 구조는 시간에 따라 정보를 저장하고 처리하는 데 사용되며, Tanh 함수는 이 정보를 효과적으로 정규화하여 네트워크의 메모리 셀에 저장할 수 있도록 돕는다.
    - 은닉층 활성화 함수: Tanh는 출력이 중심화되고 정규화되는 특성 때문에, 특히 깊은 신경망의 중간 레이어에서 유용하게 사용된다. 이는 각 레이어의 입력이 너무 크거나 작아지는 것을 방지하고, 그라디언트 소실 문제를 완화하는 데 도움을 준다.

***

#### Q) 소프트 맥스도 있지 않나? 이건 출력층에서만 사용하는 활성화 함수인가? 

맞다. 신경망의 출력층에서 사용되는 활성화 함수임. 

이 함수는 다중 클래스 분류 문제에서 특히 유용하며, 네트워크의 출력을 확률로 해석할 수 있도록 도와줌. 

특징 및 목적: 
- 형태: 소프트맥스 함수는 벡터 형태의 입력을 받아, 각 클래스에 대한 예측 확률을 출력합니다. 수학적으로는 softmax(x_i) = exp(x_i) / Σ(exp(x_j))로 정의되며, 여기서 x_i는 입력 벡터의 i번째 요소를, Σ는 모든 클래스에 대한 지수 함수의 합을 나타낸다. 
- 확률 출력: 소프트맥스는 출력값을 확률로 변환하여 각 클래스에 속할 확률을 나타내므로, 결과를 직관적으로 이해할 수 있다. 모든 출력 값의 합은 1이 되며, 각 값은 0과 1 사이의 확률로 해석된다. 
- 목적: 주로 다중 클래스 분류 문제에서 사용되어, 각 클래스에 속할 확률을 계산한다. 이를 통해, 가장 높은 확률을 가진 클래스를 모델의 예측으로 선택할 수 있다. 


사용 시기: 
- 출력층에서의 사용: 소프트맥스 함수는 일반적으로 신경망의 마지막 레이어, 즉 출력층에서 사용된다. 이는 모델이 다중 클래스 분류 문제를 해결할 때, 각 클래스에 속할 확률을 제공하기 위함임. 
- 다중 클래스 분류: 이미지 분류, 텍스트 분류, 음성 인식 등 다양한 분야에서 널리 사용된다. 예를 들어, 손글씨 숫자 인식, 뉴스 기사의 카테고리 분류, 음성 명령을 다양한 명령어로 분류하는 경우 등에 활용됨. 

***

#### Q) 실제로 커스텀 딥러닝 모델을 설계할 때 어떤 활성화 함수를 적용해야하는지 공식 같은게 있는거야? 아니면 경험에 의존하는거야? 

공식이나 규칙이 존재하는게 아님. 

경험과 실험에 의존하며 고려사항은 있음: 
- 문제의 종류:
  - 이진 분류 문제에서는 출력층에 주로 시그모이드 활성화 함수를 사용함. 
  - 다중 클래스 분류 문제에서는 출력층에 소프트맥스 활성화 함수를 사용함.
  - 회귀 문제에서는 활성화 함수 없이, 또는 선형 활성화 함수를 사용할 수 있다. 

- 데이터의 특성: 
  - 입력 데이터가 음수 값을 포함할 수 있다면, ReLU 대신 Leaky ReLU나 ELU 같은 함수가 적절할 수 있다. 

- 네트워크 구조:
  - 깊은 네트워크에서는 그래디언트 소실 문제를 방지하기 위해 ReLU 또는 그 변형들을 선호할 수 있다. 
  - 순환 신경망(RNN)이나 LSTM에서는 tanh 또는 시그모이드 함수가 자주 사용된다. 

- 실험과 조정: 
  - 모델을 처음 설계할 때는 일반적으로 잘 작동하는 활성화 함수(예: ReLU)를 시작점으로 사용한다. 
  - 이후, 네트워크의 성능을 모니터링하고, 필요에 따라 다른 활성화 함수로 실험을 해 볼 수 있다. 


다른 사람의 연구 참고: 
- 문헌 연구: 다른 연구자들이 유사한 문제에 사용한 활성화 함수를 참고할 수 있다. 이는 특히 새로운 문제에 접근할 때 유용하다. 
- 하이퍼파라미터 튜닝: 모델의 다른 하이퍼파라미터와 함께 활성화 함수도 조정 대상이다.  여러 활성화 함수를 시험해 보고, 검증 데이터셋에서 최고의 성능을 보이는 조합을 선택할 수 있다. 

***

#### Tanh 와 Sigmoid 같은 활성화 함수는 미분했을 때 기울기가 0이라서 그라디언트 소실 문제가 발생한다고 하는데 왜 일어나는거지? 

그라디언트 소실(Vanishing Gradient) 문제는 심층 신경망에서 주로 발생하는 현상임. 

이 문제가 발생하는 주된 이유는 네트워크를 통해 역전파될 때 그라디언트(가중치에 대한 손실 함수의 미분 값)가 점점 작아지기 때문이다. 

활성화 함수의 미분과 그라디언트 소실: 
- Sigmoid 함수의 특성과 미분
  - Sigmoid 함수는 σ(x) = 1 / (1 + exp(-x))로 정의됩니다. 이 함수의 출력 범위는 0과 1 사이임. 
  - Sigmoid 함수의 미분은 σ'(x) = σ(x)(1 - σ(x))입니다. 이 미분 값은 x가 0에서 최대이며, x의 절대값이 커질수록 그라디언트는 0에 가까워진다. 즉, 입력 값의 절대값이 크면 크게 낮은 그라디언트를 가짐. 
- Tanh 함수의 특성과 미분: 
  - Tanh 함수는 tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))로 정의되며, 출력 범위는 -1과 1 사이임. 
  - Tanh 함수의 미분은 tanh'(x) = 1 - tanh^2(x)입니다. 이 미분 값 역시 x의 절대값이 클수록 0에 가까워지며, 출력 범위가 -1과 1 사이인 점에서 Sigmoid 함수의 문제를 다소 완화시키긴 하지만, 깊은 네트워크에서 여전히 그라디언트 소실 문제를 유발할 수 있다. 

그라디언트 소실이 일어나는 과정: 
- 역전파에서의 그라디언트 계산: 신경망에서 손실 함수의 그라디언트는 출력층에서 입력층으로 역전파된다. 각 레이어를 지날 때마다 레이어의 가중치에 대한 미분을 현재까지의 미분 결과에 곱한다. 
- 미분값의 감소: 활성화 함수의 미분 결과가 1보다 작을 경우, 이 미분값들이 연쇄적으로 곱해지면서 그라디언트의 크기가 지수적으로 감소한다. 따라서 신경망이 깊어질수록, 특히 Sigmoid나 Tanh 같은 활성화 함수를 사용할 경우, 입력층에 가까운 레이어의 그라디언트는 거의 0에 수렴할 수 있다. 
- 가중치 업데이트의 실패: 그라디언트가 매우 작아지면, 가중치 업데이트가 제대로 이루어지지 않는다. 이는 학습이 느려지거나 전혀 이루어지지 않는 상황을 초래할 수 있으며, 결국 모델의 성능이 제한된다.


해결 방안: 
- 그라디언트 소실 문제를 해결하기 위한 몇 가지 접근 방법이 있음. 
  - ReLU 활성화 함수 사용: ReLU (Rectified Linear Unit) 함수는 그라디언트 소실 문제를 상당 부분 해결해 주며, 현재 가장 많이 사용되는 활성화 함수 중 하나임. 
  - 가중치 초기화 전략 개선: He 초기화나 Glorot 초기화와 같은 전략을 사용하여 네트워크의 초기 가중치를 적절히 설정함으로써 그라디언트 소실 문제를 완화할 수 있다. 
  - 배치 정규화: 각 레이어의 입력을 정규화하여 레이어 간에 그라디언트가 고르게 퍼지도록 돕는다.  


***

#### Q) 역전파에서의 그라디언트 계산은 어떻게 되는거지? (역전파 그라디언트 계산은 미분 값이 연쇄적으로 곱해지는거긴 한듯. 

역전파(Backpropagation)는 신경망에서 손실 함수의 그라디언트를 계산하고, 이를 사용하여 네트워크의 가중치를 조정하는 과정임. 

이 과정은 신경망의 출력층에서 시작해 입력층으로 거슬러 올라가면서 진행됨. 

역전파에서 그라디언트 계산은 연쇄 법칙(Chain Rule)을 사용하여 각 레이어의 가중치에 대한 손실 함수의 미분 값을 계산한다. 

역전파의 단계별 설명: 
- 1. 손실 함수의 정의: 
  - 신경망의 학습 목표는 손실 함수(Loss Function)를 최소화하는 것임.  이 함수는 실제 값과 예측 값 사이의 차이를 수치적으로 나타낸다. 
- 2. 순전파(Forward Propagation): 
  - 입력 데이터는 신경망의 각 레이어를 통과하면서, 각 레이어의 활성화 함수를 거쳐 변환된 결과를 출력한다. 이 과정에서 최종 출력(예측 값)이 생성됨. 
- 3. 손실 계산 
  - 신경망의 최종 출력과 실제 값과의 차이를 계산하여 손실을 도출한다. 
- 4. 그라디언트 계산과 역전파 시작
  - 손실 함수의 그라디언트를 출력층에서부터 계산하기 시작한다. 이 그라디언트는 손실 함수의 미분 값으로, 이는 출력층의 활성화 함수 미분과 결합된다. 
- 5. 연쇄 법칙의 적용
  - 연쇄 법칙에 따라, 각 레이어의 출력에 대한 손실 함수의 미분(그라디언트)는 이전 레이어의 출력에 대한 미분과 현재 레이어의 가중치에 대한 미분을 곱한 값이다. 
  - 구체적으로, 레이어 L의 가중치 W에 대한 그라디언트는 dL/dW = (dL/dy) * (dy/dW)로 계산된다.  여기서 dL/dy는 다음 레이어에서 전달된 그라디언트이고, dy/dW는 현재 레이어의 출력에 대한 가중치의 미분이다. 
- 6. 그라디언트의 역전파
  - 이 과정을 통해 각 레이어의 그라디언트가 계산되고, 입력층 방향으로 전파된다. 각 레이어를 거쳐갈 때마다 가중치를 업데이트하기 위한 그라디언트가 계산된다. 
- 7. 가중치 업데이트
  - 계산된 그라디언트를 사용하여 네트워크의 가중치를 업데이트한다.  이때 학습률(Learning Rate)이라는 파라미터를 사용하여 그라디언트의 영향을 조절한다. 


예시: 

![](./images/backpropagation%201.png)
![](./images/backpropagation%202.png)


- 계산하는 과정을 보면 해당 손실 함수의 값을 최소화 하기 위해 각 레이어의 가중치를 업데이트할 때 미분 계산은 이전 레이어의 미분 계산에서 누적되어서 온다. 

***

#### Q) 가중치 초기화가 어떻게 그라디언트 소실 문제를 막는거지? 

가중치의 크기에 따라서 그라디언트 업데이트에 어떤 영향을 주는지 먼저 알아야함. 신경망에서는 가중치를 적절하게 초기화하지 않으면 다음과 같은 문제가 발생할 수 있음: 
- 그라디언트 소실: 가중치가 너무 작게 초기화되면, 신경망의 깊은 레이어로 갈수록 그라디언트가 점차 작아져서 결국 0에 가까워짐. 이는 역전파 시 학습 신호가 사라지는 것을 의미하며, 이로 인해 깊은 레이어의 가중치가 제대로 업데이트되지 않아 학습이 제대로 이루어지지 않는 문제가 발생함. 
- 그라디언트 폭발: 반대로 가중치가 너무 크게 초기화되면, 그라디언트가 너무 커져서 역전파 시 수치적으로 불안정해질 수 있다. 이는 학습 과정에서 네트워크의 가중치 값이 발산하여 오버플로우를 일으킬 수 있음. 

그러니까 활성화 함수로 sigmoid 나 tanh 를 적용했다고 해서 가중치를 무턱대고 크게 주면 안된다. 중요한 건 각 레이어를 통과하면서 입력의 분산과 출력의 분산이 유지되게 하는거임. 

이를 위한 방법으로 Xavier/Glorot 초기화는 레이어에 들어오는 연결의 개수(입력 뉴런 수) n 을 기반으로 가중치 초기화를 결정한다. 

![](./images/xavier%20glorot%20초기화.png)

- n 이 클수록 가중치 초기화의 크기를 더 작게 만드는거임. 각 뉴런이 더 많은 입력을 받을 수 있기 때문에 전체 합산이 너무 커지지 않도록 만드는 것. 
- 이 밧익은 비선형 활성화 함수인 Sigmoid나 Tanh에서도 사용 가능하지만, ReLU에서는 그다지 효과적이지 않음. 


다른 방법으로 He 초기화 방법은 ReLU 활성화 함수와 그 변형들을 사용할 때 효과적임.  

![](./images/he%20초기화.png)

- 이것도 n 은 입력 노드의 개수다. ReLU의 특성상 음수 입력에 대해 그라디언트가 0이 되므로, 이 초기화 방법은 그라디언트 소실 문제를 줄이는 데 효과적임. 

정리하자면 가중치 초기화는 각 레이어의 입력 노드 수에 따라 결정됨. 깊은 네트워크의 경우, 이러한 기본적인 초기화만으로는 그라디언트 소실 문제를 완전히 해결하기 어려울 수 있다는 걸 알자. 

*** 

#### Q) 배치 정규화가 어떻게 그라디언트 소실 문제를 막는거지? 배치 정규화는 입력 값을 정규화해서 그라디언트 값을 안정화 하는 역할을 하는거 아니야? 
 
배치 정규화는 학습 과정 중 각 미니배치의 출력을 정규화하여, 다음과 같은 방법으로 그라디언트 소실과 폭발을 막는 데 도움을 준다: 

평균과 분산의 정규화: 각 미니배치의 평균과 분산을 계산하여, 출력 값을 평균 0과 분산 1이 되도록 조정함 이렇게 하면, 학습 과정 중 입력 값의 스케일이 크게 변동하는 것을 방지하고, 레이어를 통과하는 데이터가 안정적인 상태를 유지할 수 있음.

그라디언트의 흐름 개선: 정규화된 입력은 활성화 함수를 통과할 때 더 적절한 값의 범위에 위치하게 되어, 활성화 함수의 미분 값이 0에 가깝게 되는 것을 방지함. 예를 들어, Sigmoid나 Tanh 함수는 입력 값이 크거나 작을 때 미분 값이 거의 0에 가까워지는데, 정규화를 통해 이러한 극단적인 입력 값을 피할 수 있다. 

추가로 배치 정규화는 내부 공변량 변화 감소를 할 수 있다.  내부 공변량 변화란 학습 과정 중 네트워크의 파라미터 업데이트로 인해 레이어의 입력 분포가 변하는 현상을 의미함. 이 현상을 줄이면, 각 레이어는 더 안정적인 환경에서 학습을 진행할 수 있으며, 이는 전체 네트워크의 학습 효율을 향상시킨다. 







