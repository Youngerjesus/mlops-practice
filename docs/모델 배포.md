# 모델 배포 

## AI/ML 서비스 개발 과정 

(1) 데이터 수집, 데이터 전처리 (e.g Pandas, Google Cloud BigQuery, Spark)


(2) 모델 학습 및 평가 (e.g Pytorch, skLearn)


(3) 모덾 패키징 및 모델 서빙 (e.g mlflow, Pytorch Torch Serviing, BentoML, Huggingface, NVIDIA Serving tool, Kubeflow, AWS SageMaker, Google Cloud Vertex AI)


(4) 모니터링 및 모델 학습 파이프라인과 피처 스토어 

## TorchServe 

PyTorch 모델을 쉽게 배포하고 서빙할 수 있도록 지원하는 오픈소스 도구임 

모델을 실제 서비스 환경에서 사용하기 위해서는 모델 배포가 필요한데 이런 배포를 도와주는 도구임.

주요 기능: 
- 모델 등록 및 관리:
  - 여러 모델을 동시에 관리할 수 있으며, 버전 관리를 지원한다. 
  - 모델의 다양한 메타데이터와 설정을 포함한 모델 저장소를 관리할 수 있다. 
- RESTful API 제공:
  - RESTful API를 통해 모델 예측을 쉽게 요청할 수 있다. 
  - JSON 형식의 요청 및 응답을 지원한다. 
- 자동 스케일링:
  - 트래픽 증가에 따라 자동으로 인스턴스를 스케일 업하거나 스케일 다운할 수 있다. 
  - 로드 밸런싱 기능을 통해 안정적인 서비스 제공이 가능하다. 
- 모니터링 및 로깅:
  - 모델의 성능과 상태를 모니터링할 수 있는 다양한 메트릭을 제공한다. 
  - 로그 파일을 통해 모델의 예측 결과와 오류를 추적할 수 있다. 
- 쉽고 빠른 배포:
  - Docker와 Kubernetes와 같은 컨테이너화된 환경에서 쉽게 배포할 수 있다. 
  - 클라우드 환경뿐만 아니라 온프레미스 환경에서도 유연하게 사용할 수 있다.


#### Q) 보통은 API 제공은 TorchServe 가 아니라 FastAPI 를 쓰지?

그렇다. 

FastAPI는 비동기 처리를 지원하여 고성능의 API 서버를 구축할 수 있어서 그럼.

다만 TorchServe는 PyTorch 모델 서빙에 특화된 기능들을 제공하여 배포 및 관리가 용이함.

단순한 모델 서빙이 필요하고 PyTorch 모델에 집중하고 싶다면 TorchServe가 좋을 수 있지만, 모델 서빙 외에도 다양한 API를 제공해야 한다면 FastAPI를 사용하는 것이 더 적합할 수 있음. 


#### Q) TorchServe 의 모델 서빙 특화 기능이란? 

모델 관리: 
- 모델 저장소: 모델을 저장하고 버전 관리할 수 있는 모델 저장소 기능을 제공한다. 이는 여러 모델을 동시에 관리하고 배포할 때 유용하다. 
- 모델 레지스트리: 모델 아카이브(.mar) 파일을 생성하여 모델, 전처리 및 후처리 코드, 설정 등을 포함시킬 수 있다. 

API 제공:
- RESTful 및 gRPC API: HTTP와 gRPC 프로토콜을 통해 모델 예측 요청을 처리할 수 있는 API를 제공한다. 이는 다양한 클라이언트와의 통합을 용이하게 한다.

자동 스케일링 및 로드 밸런싱
- 자동 스케일링: 트래픽에 따라 자동으로 인스턴스를 스케일 업 또는 스케일 다운할 수 있다. 이는 고가용성과 성능 유지를 도움
- 로드 밸런싱: 여러 인스턴스 간에 트래픽을 분산시켜 고른 부하 분산을 제공한다. 

모니터링 및 로깅
- 모니터링: 모델의 성능, 사용률, 응답 시간 등의 메트릭을 제공하여 모델 상태를 모니터링할 수 있다.
- 로깅: 요청 및 응답 로그, 에러 로그 등을 기록하여 문제를 추적하고 디버깅할 수 있다. 

배치 처리
- 배치 예측: 다수의 예측 요청을 하나의 배치로 묶어 처리할 수 있는 기능을 제공한다. 이는 처리 속도를 향상시키고, 리소스 효율성을 높인다. 

A/B 테스트 및 모델 전환
- A/B 테스트: 여러 모델을 동시에 배포하여 성능을 비교할 수 있다. 이를 통해 최적의 모델을 선택할 수 있다.
- 모델 전환: 새로운 모델 버전으로의 전환을 원활하게 할 수 있는 기능을 제공하여 다운타임 없이 모델을 업데이트할 수 있다. 


다양한 배포 옵션: 
- Docker 및 Kubernetes: Docker 컨테이너와 Kubernetes를 통해 손쉽게 배포하고 관리할 수 있다. 이는 클라우드 환경뿐만 아니라 온프레미스 환경에서도 유연하게 사용할 수 있다. 


## 예제: TorchServe 로 모델 서빙 

### 모델 서빙 코드부터 

```python
# 모델 체크포인트 저장.
# 학습된 모델의 가중치를 저장할 파일 경로를 지정한다. 여기서는 "bert_news_classification_model.pth"라는 파일명을 사용한다. 
model_save_path = "bert_news_classification_model.pth"

# model.state_dict()를 사용하여 모델의 가중치(state dict)를 추출하고, torch.save를 통해 지정된 경로에 저장한다. 
# 이는 모델을 나중에 다시 로드할 수 있도록 한다. 
torch.save(model.state_dict(), model_save_path)


# model_handler.py
# Jupyter 노트북에서 셸 명령어를 실행할 수 있게 하는 매직 명령어임. 
%%shell
cat > model_handler.py <<EOF

# JSON 데이터를 처리하기 위한 모듈을 가져온다.
import json

# PyTorch 라이브러리를 가져온다. 
import torch

# TorchServe의 컨텍스트 객체를 가져온다. 
from ts.context import Context

# 사용자 정의 모델 핸들러를 만들기 위해 TorchServe의 기본 핸들러 클래스를 가져온다. 
from ts.torch_handler.base_handler import BaseHandler

# Hugging Face Transformers 라이브러리에서 BERT 토크나이저와 BERT 시퀀스 분류 모델을 가져온다. 
from transformers import BatchEncoding, BertTokenizer, BertForSequenceClassification

# BaseHandler를 상속받아 ModelHandler 클래스를 정의한다.
# ModelHandler 클래스는 사용자의 요청을 받아 모델이 예측을 수행하도록 만드는 클래스이다.
# 모델 초기화, 추론을 위한 입력 데이터 전처리, 추론, 모델 예측 결과를 이용해서 사용자에게 반환하도록 하는 후처리 이런 기능을 대표적으로 제공한다. 
class ModelHandler(BaseHandler):
    
    def __init__(self):
        # 모델 핸들러가 초기화되었는지 여부를 나타내는 플래그를 설정한다. 
        self.initialized = False
        
        # BERT 토크나이저 객체를 저장할 변수를 초기화한다. 
        self.tokenizer = None
        
        # BERT 모델 객체를 저장할 변수를 초기화한다. 
        self.model = None

    # 모델 핸들러를 초기화하는 메서드로, TorchServe가 모델을 로드할 때 호출된다.
    def initialize(self, context: Context):
        # 모델 핸들러가 초기화되었음을 표시한다. 
        self.initialized = True
        
        # 사전학습된 BERT 토크나이저를 로드한다. 
        self.tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
        
        # 사전학습된 BERT 모델을 로드하고, 분류할 레이블의 수를 4로 설정한다. 
        self.model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=4)
        
        #  저장된 모델 가중치를 로드한다. 
        self.model.load_state_dict(torch.load("bert_news_classification_model.pth"))
        
        # 모델을 GPU(가능한 경우) 또는 CPU에 로드한다. 
        self.model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
        
        # 모델을 평가 모드로 전환하여 드롭아웃 같은 레이어가 비활성화되도록 한다. 
        self.model.eval()

    # 입력 데이터를 전처리하는 메서드이다. 
    def preprocess(self, data: list[dict[str, bytearray]]) -> BatchEncoding:
        # 입력 데이터에서 텍스트를 추출하여 리스트로 만든다. 
        model_input_texts: list[str] = sum([json.loads(item.get("body").decode("utf-8"))["data"] for item in data], [])
        
        # 토크나이저를 사용하여 텍스트를 토큰으로 변환하고, 패딩과 자르기(truncation)를 수행하며, PyTorch 텐서 형식으로 반환한다. 
        inputs = self.tokenizer(model_input_texts, truncation=True, padding=True, max_length=512, return_tensors="pt")
        
        # 사용 가능한 디바이스(GPU 또는 CPU)를 설정한다. 
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 전처리된 입력을 설정된 디바이스에 로드하여 반환한다. 
        return inputs.to(device)

    # 전처리된 입력 데이터를 바탕으로 예측을 수행하는 메서드이다. 
    def inference(self, input_batch: BatchEncoding) -> torch.Tensor:
        # 그라디언트 계산을 비활성화하여 메모리 사용을 줄이고, 추론(inference) 성능을 향상시킨다. 
        with torch.no_grad():
            # 모델을 사용하여 입력 데이터에 대한 예측을 수행한다. 
            outputs = self.model(**input_batch)
            
            # 모델의 로짓(logits)을 반환한다. 로짓은 소프트맥스 함수 적용 전의 원시 예측 값이다. 
            return outputs.logits

    # 모델 예측 결과를 후처리하는 메서드이다. 
    def postprocess(self, inference_output: torch.Tensor) -> list[dict[str, float]]:
        # 로짓에 소프트맥스 함수를 적용하여 각 클래스에 대한 확률을 계산한다.
        # torch.nn.functional.softmax: PyTorch에서 제공하는 소프트맥스 함수이다. 소프트맥스 함수는 입력 값(로짓)을 확률로 변환한다.
        # inference_output: 모델의 출력인 로짓(logits)을 입력으로 받는다. 
        # probabilities: 소프트맥스 함수를 적용한 결과로, 각 클래스에 대한 확률이 담긴 텐서이다.
        # dim=1: 소프트맥스 함수를 적용할 차원을 지정한다. 여기서는 각 샘플에 대한 클래스 차원에 소프트맥스 함수를 적용한다. 
        probabilities = torch.nn.functional.softmax(inference_output, dim=1)
        
        # 각 클래스의 확률 중 가장 높은 값을 가진 클래스를 라벨로 지정하고, 해당 확률과 함께 반환한다.
        # 리스트 컴프리헨션을 사용하여 각 샘플에 대한 예측 결과를 처리한다. 
        # prob: 소프트맥스 함수의 결과로, 각 클래스에 대한 확률을 담고 있는 텐서이다. 
        # torch.argmax(prob): 가장 높은 확률을 가진 클래스의 인덱스를 반환한다. 
        # prob.max(): 가장 높은 확률 값을 반환한다. 
        # {"label": int(torch.argmax(prob)), "probability": float(prob.max())}: 예측된 클래스 라벨과 해당 확률을 딕셔너리 형태로 변환한다. 
        return [{"label": int(torch.argmax(prob)), "probability": float(prob.max())} for prob in probabilities]
EOF
```

TorchServe 설정 파일 작성
- config.properties 파일은 추론, 관리, 메트릭스 주소를 설정한다.
- Inference Address: 모델 추론 요청을 처리한다. 클라이언트 애플리케이션이 모델 예측을 요청할 때 이 주소를 사용한다. 
- Management Address: 모델 서버의 관리 작업을 처리한다. 모델 등록, 모델 배포 상태 확인, 로그 조회 등
- Metrics Address: 성능 모니터링 및 메트릭스를 제공한다. 서버의 성능 지표(예: 요청 수, 응답 시간, CPU/메모리 사용량 등)를 수집하고 제공하는 데 사용된다. 
- TorchServe 는 TorchServe 는 설정 파일에서 지정된 주소와 포트를 기반으로 서버를 구성한다. 
- inference_address=http://0.0.0.0:5000 이 설정은 자신의 서버로 들어오는 모든 5000번 포트대의 요청을 수신할 수 있다는거임. 


```python
# 3. torch serve 설정 파일.
%%shell
cat > config.properties <<EOF
inference_address=http://0.0.0.0:5000
management_address=http://0.0.0.0:5001
metrics_address=http://0.0.0.0:5002
EOF
``` 

BERT vocab 파일 다운로드
- BERT 모델에 필요한 단어 사전 파일을 다운로드한다. 
- wget 명령을 사용하여 BERT 모델의 단어 사전 파일을 다운로드하고 저장한다. 
- 단어 사전은 입력 텍스트를 토큰으로, 토큰을 텍스트로 변환하는데 사용한다. 이 역할은 토크나이저가 하지만 토크나이저는 단어 사전이 필요함. 
- 모델을 훈련할 때는 내부적으로 단어 사전을 사용하고 있을거임. 모델 서빙 시에도 동일한 단어 사전을 사용하는게 필요함.  
- `self.tokenizer = BertTokenizer.from_pretrained("bert-base-uncased", vocab_file="bert-base-uncased-vocab.txt")` 이렇게 하면 토크나이저가 단어 사전을 명시적으로 사용할 수 있음. 


```shell
# 4. bert vocab 파일 (아티팩트)

!wget https://raw.githubusercontent.com/microsoft/SDNet/master/bert_vocab_files/bert-base-uncased-vocab.txt \
    -O bert-base-uncased-vocab.txt

```

모델 아카이브 파일(.mar) 생성
- 모델 아카이브 파일(.mar)을 생성하여 모델과 핸들러, 추가 파일을 패키징한다. 
- torch-model-archiver: 모델을 패키징하는 도구임. 
- --model-name: 모델의 이름을 지정함. 
- --version: 모델 버전을 지정합니다.
- --serialized-file: 저장된 모델 체크포인트 파일을 지정함. 
- --handler: 모델 핸들러 파일을 지정함. 
- --extra-files: 추가 파일(여기서는 BERT vocab 파일)을 지정한다 
- --export-path: 패키징된 모델을 저장할 경로를 지정한다. 
- -f: 기존 파일을 덮어쓴다. 

```shell
# 5. Torch serve archiver의 최종 파일 저장 위치..mar 확장자 파일을 생성하여 패키징 하게 된다.
!mkdir -p model-store

!torch-model-archiver \
    --model-name bert_news_classification \
    --version 1.0 \
    --serialized-file bert_news_classification_model.pth \
    --handler ./model_handler.py \
    --extra-files "bert-base-uncased-vocab.txt" \
    --export-path model-store \
    -f

```


TorchServe 서버 시작
- TorchServe 서버를 시작하여 모델을 서빙한다. 
- PYTHONPATH=/usr/lib/python3.10: Python 경로를 설정한다. 
- torchserve --start: TorchServe 서버를 시작한다. 
- --ncs: 서버를 노 캐치 서버로 실행한다. 
- --ts-config: 설정 파일 경로를 지정한다. 
- --model-store: 모델 저장소 경로를 지정한다.
- --models: 배포할 모델을 지정한다. 


```shell
%%script bash --bg
# 6. 서버를 백그라운드에서 실행

PYTHONPATH=/usr/lib/python3.10 torchserve \
    --start \
    --ncs \
    --ts-config config.properties \
    --model-store model-store \
    --models bert_news_classification=bert_news_classification.mar

```

서버 상태 확인: 
- curl 명령을 사용하여 서버 상태를 확인하는 요청을 보낸다. 

```shell
# 7.  아래와 같이 뜨면 정상 실행.
# {
#   "status": "Healthy"
# }

!curl -X GET localhost:5000/ping
```

모델 실제 평가를 위해 외부 뉴스 기사 데이터 셋 파일 생성
```shell
%%shell.

cat > request_sports.json <<EOF
{
  "data": [
    "Bleary-eyed from 16 hours on a Greyhound bus, he strolled into the stadium running on fumes. He’d barely slept in two days. The ride he was supposed to hitch from Charlotte to Indianapolis canceled at the last minute, and for a few nervy hours, Antonio Barnes started to have his doubts. The trip he’d waited 40 years for looked like it wasn’t going to happen.ADVERTISEMENTBut as he moved through the concourse at Lucas Oil Stadium an hour before the Colts faced the Raiders, it started to sink in. His pace quickened. His eyes widened. His voice picked up.“I got chills right now,” he said. “Chills.”Barnes, 57, is a lifer, a Colts fan since the Baltimore days. He wore No. 25 on his pee wee football team because that’s the number Nesby Glasgow wore on Sundays. He was a talent in his own right, too: one of his old coaches nicknamed him “Bird” because of his speed with the ball.Back then, he’d catch the city bus to Memorial Stadium, buy a bleacher ticket for $5 and watch Glasgow and Bert Jones, Curtis Dickey and Glenn Doughty. When he didn’t have any money, he’d find a hole in the fence and sneak in. After the game was over, he’d weasel his way onto the field and try to meet the players. “They were tall as trees,” he remembers.He remembers the last game he went to: Sept. 25, 1983, an overtime win over the Bears. Six months later the Colts would ditch Baltimore in the middle of the night, a sucker-punch some in the city never got over. But Barnes couldn’t quit them. When his entire family became Ravens fans, he refused. “The Colts are all I know,” he says.For years, when he couldn’t watch the games, he’d try the radio. And when that didn’t work, he’d follow the scroll at the bottom of a screen.“There were so many nights I’d just sit there in my cell, picturing what it’d be like to go to another game,” he says. “But you’re left with that thought that keeps running through your mind: I’m never getting out.”It’s hard to dream when you’re serving a life sentence for conspiracy to commit murder.It started with a handoff, a low-level dealer named Mickey Poole telling him to tuck a Ziploc full of heroin into his pocket and hide behind the Murphy towers. This was how young drug runners were groomed in Baltimore in the late 1970s. This was Barnes’ way in.ADVERTISEMENTHe was 12.Back then he idolized the Mickey Pooles of the world, the older kids who drove the shiny cars, wore the flashy jewelry, had the girls on their arms and made any working stiff punching a clock from 9 to 5 look like a fool. They owned the streets. Barnes wanted to own them, too.“In our world,” says his nephew Demon Brown, “the only successful people we saw were selling drugs and carrying guns.”So whenever Mickey would signal for a vial or two, Barnes would hurry over from his hiding spot with that Ziploc bag, out of breath because he’d been running so hard."
  ]
}
EOF

cat > request_business.json <<EOF
{
  "data": [
    "DETROIT – America maintained its love affair with pickup trucks in 2023 — but a top-selling vehicle from Toyota Motor nearly ruined their tailgate party.Sales of the Toyota RAV4 compact crossover came within 10,000 units of Stellantis’ Ram pickup truck last year, a near-No. 3 ranking that would have marked the first time since 2014 that a non-pickup claimed one of the top three U.S. sales podium positions.The RAV4 has rapidly closed the gap: In 2020, the vehicle undersold the Ram truck by more than 133,000 units. Last year, it lagged by just 9,983. Stellantis sold 444,926 Ram pickups last year, a 5% decline from 2022.“Trucks are always at the top because they’re bought by not only individuals, but also fleet buyers and we saw heavy fleet buying last year,” said Michelle Krebs, an executive analyst at Cox Automotive. “The RAV4 shows that people want affordable, smaller SUVs, and the fact that there’s also a hybrid version of that makes it popular with people.”"
  ]
}
EOF

cat > request_sci_tech.json <<EOF
{
  "data": [
    "OpenVoice comprises two AI models working together for text-to-speech conversion and voice tone cloning.The first model handles language style, accents, emotion, and other speech patterns. It was trained on 30,000 audio samples with varying emotions from English, Chinese, and Japanese speakers. The second “tone converter” model learned from over 300,000 samples encompassing 20,000 voices.By combining the universal speech model with a user-provided voice sample, OpenVoice can clone voices with very little data. This helps it generate cloned speech significantly faster than alternatives like Meta’s Voicebox.Californian startup OpenVoice comes from California-based startup MyShell, founded in 2023. With $5.6 million in early funding and over 400,000 users already, MyShell bills itself as a decentralised platform for creating and discovering AI apps.  In addition to pioneering instant voice cloning, MyShell offers original text-based chatbot personalities, meme generators, user-created text RPGs, and more. Some content is locked behind a subscription fee. The company also charges bot creators to promote their bots on its platform.By open-sourcing its voice cloning capabilities through HuggingFace while monetising its broader app ecosystem, MyShell stands to increase users across both while advancing an open model of AI development."
  ]
}
EOF
```

스포츠 기사 평가 (레이블: 1)

```shell
!curl -X POST \
    -H "Accept: application/json" \
    -T "request_sports.json" \
    http://localhost:5000/predictions/bert_news_classification
```

비즈니스 기사 평가 (레이블: 2)

```shell
!curl -X POST \
    -H "Accept: application/json" \
    -T "request_business.json" \
    http://localhost:5000/predictions/bert_news_classification
```

